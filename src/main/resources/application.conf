{
  s3aUri: "s3a://"
  s3Uri: "s3://"
  s3Bucket: "indeed-data"
  basePath: "datalake/v1/prod/mysql"
  checkpointBaseLoc: "/spark/streaming/checkpoint/"
  kafka {
    brokers: "172.31.15.18:9092,172.31.10.175:9092"
    //    brokers: "ec2-54-160-85-68.compute-1.amazonaws.com:9092,ec2-54-226-72-146.compute-1.amazonaws.com:9092"
    topic: "maxwell"
  }

  cassandra {
    keyspace: adcentraldb
    host: "172.31.10.148,172.31.15.185,172.31.15.43,172.31.4.148"
    //    host: "34.230.53.208,52.90.234.117,52.90.234.213,107.23.201.226"
  }

  MergeS3ToRedshift {
    processName: historicalBuild
    jobType: history
    jobGroup: source
    jobFrequency: streaming
    description: "copy and merge data from s3 to redshift to construct historical source tables"
  }
}